{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Felipe Schiavinato\n",
    "\n",
    "Nome: Jerônimo Afrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "/Users/jeronimo/Desktop/Insper/2/CDADOS/p1-cdados\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados como relevantes e não relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'bitcoin.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>relevancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>do you think it is ethical of exchanges to ear...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>@junkosu22993224: citi bank is the first major...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>@wsbmod: it's time to decentralize wallstreetb...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>@iamlluciana charlie the bitcoin fund manager ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>#btc #bitcoin #forex\\nus stocks set for swoon ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>#bitcoin is falling below 56,000 $ usd</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>@coinmarketcap your #bitcoin price is showing ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>@wealth_theory: i cannot understand how you ca...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>@themooncarl: #bitcoin at $100,000 doesn’t see...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets  relevancia\n",
       "0    do you think it is ethical of exchanges to ear...         3.0\n",
       "1    @junkosu22993224: citi bank is the first major...         3.0\n",
       "2    @wsbmod: it's time to decentralize wallstreetb...         2.0\n",
       "3    @iamlluciana charlie the bitcoin fund manager ...         1.0\n",
       "4    #btc #bitcoin #forex\\nus stocks set for swoon ...         0.0\n",
       "..                                                 ...         ...\n",
       "746             #bitcoin is falling below 56,000 $ usd         2.0\n",
       "747  @coinmarketcap your #bitcoin price is showing ...         3.0\n",
       "748  @wealth_theory: i cannot understand how you ca...         3.0\n",
       "749  @themooncarl: #bitcoin at $100,000 doesn’t see...         3.0\n",
       "750                                                NaN         NaN\n",
       "\n",
       "[751 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_treinamento = pd.read_excel(filename)\n",
    "dados_treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                 tweets  relevancia\n",
       "0    @phorecrypto: a snapshot of #phore blockchain ...         2.0\n",
       "1    @riena_smile: bottlepay launches twitter bitco...         3.0\n",
       "2    @btc_archive: #bitcoin on target for 75k  in t...         3.0\n",
       "3    @grady_booch at this stage it seems vanishingl...         3.0\n",
       "4    @ragnarly i have 3 kids. simply cant ask aroun...         2.0\n",
       "..                                                 ...         ...\n",
       "496  ethereum could replace bitcoin to become top c...         3.0\n",
       "497  will #paint #abstract 4 #crypto #bitcoin  #eth...         0.0\n",
       "498  i don’t agree with gary on bitcoin but i total...         2.0\n",
       "499  @meanhash: you either get it, or you don't. #b...         2.0\n",
       "500                                                NaN         NaN\n",
       "\n",
       "[501 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_teste = pd.read_excel(filename, sheet_name = 'Teste')\n",
    "dados_teste.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça aqui uma descrição do seu produto e o que considerou como relevante ou não relevante na classificação dos tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O nosso produto é o Bitcoin, e temos como objetivo analisar a opinião do público a respeito dessa criptomoeda específica. Classificamos a relevância dos Tweets com uma nota de 0 a 4, sendo 0 completamente irrelevante e 4 completamente relevante.\n",
    "\n",
    "Consideramos como completamente irrelevantes Tweets que mencionam o nome Bitcoin mas não estão relacionados à criptomoeda, assim como Tweets de zombaria.\n",
    "\n",
    "Consideramos como completamente relevantes Tweets que mencionam a criptomoeda, sejam, de fato, a respeito do nosso produto e autênticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementos bloqueados\n",
    "blacklist = \"[!-—.:?;,#…']•()1234567890%$/\\’+*\" + '\"“”'\n",
    "\n",
    "# palavras inúteis\n",
    "with open('useless_words.txt') as file:\n",
    "    palavras_inuteis = file.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def treinar(dados_treinamento):\n",
    "    '''\n",
    "        Treina o algorítmo\n",
    "        dados_treinamento: DataFrame com relevâncias e tweets\n",
    "    '''\n",
    "    \n",
    "    # lista de palavras extraídas\n",
    "    palavras_por_relevancia = [[],[],[],[],[]]\n",
    "\n",
    "    tweets = dados_treinamento['tweets']\n",
    "    relevancias = dados_treinamento['relevancia']\n",
    "    \n",
    "    # loop que passa tweet por tweet\n",
    "    for indice in range(len(tweets)):\n",
    "\n",
    "        # cria constantes 'tweet_original' e 'relevancia'\n",
    "        try: relevancia = int(relevancias[indice])\n",
    "        except: continue\n",
    "        tweet_original = tweets[indice]\n",
    "\n",
    "        tweet_limpo = cleanup(tweet_original)\n",
    "\n",
    "        # adicionando as palavras à lista de palavras da respectiva relevância\n",
    "        palavras_por_relevancia[relevancia] += tweet_limpo\n",
    "    \n",
    "    # define a lista que conterá as series das frequências normalizadas\n",
    "    # das palavras para cada grau de relevancia\n",
    "    tabelas_relativas_por_relevancia = list()\n",
    "    palavras_ingles = list()\n",
    "\n",
    "    for palavras in palavras_por_relevancia:\n",
    "        tabelas_relativas_por_relevancia.append(pd.Series(palavras).value_counts(True))\n",
    "        palavras_ingles += palavras\n",
    "\n",
    "    tabela_ingles_relativa = pd.Series(palavras_ingles).value_counts(True)\n",
    "        \n",
    "    return tabelas_relativas_por_relevancia, tabela_ingles_relativa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanup(tweet_original):\n",
    "    \n",
    "    # remove links e nomes de usuário\n",
    "    tweet_lista = tweet_original.split()\n",
    "    tweet_sem_nomes_links = list()\n",
    "    \n",
    "    for palavra in tweet_lista:\n",
    "        \n",
    "        # remove letras maiúsculas\n",
    "        palavra = palavra.lower()\n",
    "                \n",
    "        not_link = 'http' not in palavra\n",
    "        not_usuario = '@' not in palavra\n",
    "        \n",
    "        if not_link and not_usuario:\n",
    "            tweet_sem_nomes_links.append(palavra)\n",
    "        \n",
    "    tweet = ' '.join(tweet_sem_nomes_links)\n",
    "    \n",
    "    '''\n",
    "    *** a partir desse ponto, 'tweet' não tem mais nomes de usuário nem links\n",
    "    '''\n",
    "    \n",
    "    # remove caracteres bloqueados\n",
    "    for carac in blacklist:\n",
    "        \n",
    "        if carac in tweet:\n",
    "            tweet = tweet.split(carac)\n",
    "            tweet = ' '.join(tweet)\n",
    "                \n",
    "    '''\n",
    "    *** a partir desse ponto, 'tweet' não tem mais caracteres bloqueados\n",
    "    '''\n",
    "        \n",
    "    # remove palavras inúteis\n",
    "    tweet = tweet.split()\n",
    "    \n",
    "    for inutil in palavras_inuteis:\n",
    "        while inutil in tweet:\n",
    "            tweet.remove(inutil)\n",
    "    \n",
    "    tweet = ' '.join(tweet)\n",
    "            \n",
    "    '''\n",
    "    *** a partir desse ponto, 'tweet' não tem mais palavras inúteis\n",
    "    '''\n",
    "                \n",
    "    # remove espaços em branco duplos\n",
    "    tweet = tweet.split()\n",
    "    while '' in tweet: tweet.remove('')\n",
    "    tweet = ' '.join(tweet)\n",
    "    \n",
    "    '''\n",
    "    *** a partir desse ponto, 'tweet' não tem espaços em branco repetidos\n",
    "    '''\n",
    "    \n",
    "    return tweet.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tabelas_relevancias_inicial, tabela_ingles_inicial = treinar(dados_treinamento)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def testar(dados_teste, tabelas_relevancias):\n",
    "    \n",
    "    tweets_testes = list(dados_teste['tweets'])\n",
    "    relevancias = list(dados_teste['relevancia'])\n",
    "\n",
    "    certo_errado = list()\n",
    "    menor_maior = list()\n",
    "\n",
    "    for indice in range(len(dados_teste)): \n",
    "\n",
    "        lista_frase_limpa = cleanup(str(tweets_testes[indice]))\n",
    "        relevancia_real = relevancias[indice]\n",
    "\n",
    "        # Probabilidades de uma palavra qualquer pertencer a um grupo de relevância específico\n",
    "\n",
    "        lens_relevancias = list()\n",
    "\n",
    "        for palavras in tabelas_relevancias:\n",
    "            lens_relevancias.append(len(palavras))\n",
    "\n",
    "        len_total = sum(lens_relevancias)\n",
    "\n",
    "        probabilidades_individuais = list()\n",
    "\n",
    "        for indice in range(5):\n",
    "            probabilidade = lens_relevancias[indice] / len_total\n",
    "            probabilidades_individuais.append(probabilidade)\n",
    "\n",
    "        # Cálculo da probalidade da frase aparecer dado um grau de relevância específico\n",
    "\n",
    "        probabilidades_frase_dada_relevancia = [1] * 5\n",
    "\n",
    "        for indice in range(5):\n",
    "\n",
    "            for palavra in lista_frase_limpa:\n",
    "\n",
    "                try: probPalavraDadaRelevancia = tabelas_relevancias[indice][palavra]\n",
    "                except KeyError: probPalavraDadaRelevancia = 0\n",
    "\n",
    "                vezes_palavra_aparece_na_relevancia = probPalavraDadaRelevancia * lens_relevancias[indice]\n",
    "                probPalavraDadaRelevanciaPosLaplace = (vezes_palavra_aparece_na_relevancia + 1) / (lens_relevancias[indice] + len_total)\n",
    "\n",
    "                probabilidades_frase_dada_relevancia[indice] *= probPalavraDadaRelevanciaPosLaplace\n",
    "\n",
    "\n",
    "        maior_probabilidade_encontrada = max(probabilidades_frase_dada_relevancia)\n",
    "        relevancia_mais_provavel = probabilidades_frase_dada_relevancia.index(maior_probabilidade_encontrada)\n",
    "\n",
    "        try:\n",
    "            certo_errado.append(relevancia_mais_provavel == int(relevancia_real))\n",
    "            if relevancia_mais_provavel > relevancia_real: menor_maior.append('+')\n",
    "            else: menor_maior.append('-')\n",
    "        except: continue\n",
    "\n",
    "    resultados = list()\n",
    "\n",
    "    for indice in range(len(certo_errado)):\n",
    "\n",
    "        if certo_errado[indice]: # se resultado correto\n",
    "            resultados.append('verdadeiro')\n",
    "\n",
    "        else:\n",
    "\n",
    "            if menor_maior[indice] == '+':\n",
    "                resultados.append('falso positivo')\n",
    "\n",
    "            else:\n",
    "                resultados.append('falso negativo')\n",
    "\n",
    "\n",
    "    resultados = pd.Series(resultados).value_counts(True)\n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "verdadeiro        0.415323\n",
       "falso positivo    0.296371\n",
       "falso negativo    0.288306\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultados_teste = testar(dados_teste, tabelas_relevancias_inicial)\n",
    "resultados_teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Interpretação dos resultados:</b>\n",
    "\n",
    "Mensagens com dupla negação e/ou sarcasmo não são consideradas pelo algorítmo, já que ele interpreta a relevância das mensagens pelas palavras individualmente. Palavras por si só não têm significados semânticos, tais como sarcasmo e dupla negação.\n",
    "    \n",
    "Não é benéfico treinar o algorítmo com dados produzidos por ele mesmo porque sua taxa de acerto dificilmente será de 100%, com isso, propagar-se-iam dados incorretos, diminuindo ainda mais a taxa de acerto.\n",
    "\n",
    "Outros possíveis usos para o classificador de Naive Bayes seriam: implementação de um sistema de diagnóstico médico automatizado e sistema de decisão de carros autônomos.\n",
    "\n",
    "Uma possível melhora no classificador seria agrupar palavras semelhantes em uma palavra só, por exemplo, fazer com que palavras do mesmo verbo sob diferentes conjugações sejam consideradas iguais. Outra melhoria possível seria considerar combinações de palavras frequentes, tais como \"show de bola\" ou \"bom dia\". Uma terceira, penalizar palavras que aparecem frequentemente em textos relevantes e irrelevantes; já que essas palavras não convém muita informação por serem muito comuns em ambos os lados.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lemmatisation\n",
    "\n",
    "https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#n-grams\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.39595959595959596\n",
      "1: 0.4032258064516129\n",
      "2: 0.42828282828282827\n",
      "3: 0.4044265593561368\n",
      "4: 0.4262626262626263\n",
      "5: 0.4245472837022133\n",
      "6: 0.4666666666666667\n",
      "7: 0.4032258064516129\n",
      "8: 0.43775100401606426\n",
      "9: 0.4334677419354839\n",
      "10: 0.4314516129032258\n",
      "11: 0.4173387096774194\n",
      "12: 0.46060606060606063\n",
      "13: 0.41330645161290325\n",
      "14: 0.41967871485943775\n",
      "15: 0.40963855421686746\n",
      "16: 0.3951612903225806\n",
      "17: 0.4718875502008032\n",
      "18: 0.386317907444668\n",
      "19: 0.4383838383838384\n",
      "20: 0.4202020202020202\n",
      "21: 0.41851106639839036\n",
      "22: 0.40562248995983935\n",
      "23: 0.4433198380566802\n",
      "24: 0.39878542510121456\n",
      "25: 0.4305835010060362\n",
      "26: 0.4305835010060362\n",
      "27: 0.4303030303030303\n",
      "28: 0.3878787878787879\n",
      "29: 0.40606060606060607\n",
      "30: 0.4303030303030303\n",
      "31: 0.41935483870967744\n",
      "32: 0.38832997987927564\n",
      "33: 0.41414141414141414\n",
      "34: 0.45180722891566266\n",
      "35: 0.41330645161290325\n",
      "36: 0.3951612903225806\n",
      "37: 0.44779116465863456\n",
      "38: 0.4274193548387097\n",
      "39: 0.43259557344064387\n",
      "40: 0.41365461847389556\n",
      "41: 0.4274193548387097\n",
      "42: 0.40763052208835343\n",
      "43: 0.41448692152917505\n",
      "44: 0.43951612903225806\n",
      "45: 0.4303030303030303\n",
      "46: 0.3967611336032389\n",
      "47: 0.4089068825910931\n",
      "48: 0.4254032258064516\n",
      "49: 0.402020202020202\n",
      "50: 0.43548387096774194\n",
      "51: 0.43636363636363634\n",
      "52: 0.4426559356136821\n",
      "53: 0.45875251509054327\n",
      "54: 0.4225352112676056\n",
      "55: 0.41818181818181815\n",
      "56: 0.45454545454545453\n",
      "57: 0.43775100401606426\n",
      "58: 0.4314516129032258\n",
      "59: 0.4092741935483871\n",
      "60: 0.42655935613682094\n",
      "61: 0.4311740890688259\n",
      "62: 0.42369477911646586\n",
      "63: 0.42369477911646586\n",
      "64: 0.4228456913827655\n",
      "65: 0.42338709677419356\n",
      "66: 0.3903420523138833\n",
      "67: 0.4262626262626263\n",
      "68: 0.42655935613682094\n",
      "69: 0.42052313883299797\n",
      "70: 0.40606060606060607\n",
      "71: 0.4314516129032258\n",
      "72: 0.3951612903225806\n",
      "73: 0.41566265060240964\n",
      "74: 0.4112903225806452\n",
      "75: 0.4426559356136821\n",
      "76: 0.40725806451612906\n",
      "77: 0.4334677419354839\n",
      "78: 0.43259557344064387\n",
      "79: 0.4647887323943662\n",
      "80: 0.4153225806451613\n",
      "81: 0.40725806451612906\n",
      "82: 0.41448692152917505\n",
      "83: 0.39919354838709675\n",
      "84: 0.3870967741935484\n",
      "85: 0.4291497975708502\n",
      "86: 0.41700404858299595\n",
      "87: 0.41330645161290325\n",
      "88: 0.42857142857142855\n",
      "89: 0.41566265060240964\n",
      "90: 0.4084507042253521\n",
      "91: 0.42424242424242425\n",
      "92: 0.4303030303030303\n",
      "93: 0.41967871485943775\n",
      "94: 0.39156626506024095\n",
      "95: 0.3807615230460922\n",
      "96: 0.4008097165991903\n",
      "97: 0.4507042253521127\n",
      "98: 0.41935483870967744\n",
      "99: 0.38866396761133604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweets_teste = pd.DataFrame()\n",
    "tweets_teste['tweets'] = dados_teste['tweets']\n",
    "tweets_teste['relevancia'] = dados_teste['relevancia']\n",
    "\n",
    "tweets_treinamento = pd.DataFrame()\n",
    "tweets_treinamento['tweets'] = dados_treinamento['tweets']\n",
    "tweets_treinamento['relevancia'] = dados_treinamento['relevancia']\n",
    "\n",
    "todos_dados = pd.concat([tweets_teste, tweets_treinamento], sort=True)\n",
    "\n",
    "\n",
    "# loop de treinamento e teste :)\n",
    "\n",
    "taxas_acerto = list()\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    tweets_embaralhados = todos_dados.sample(frac=1)\n",
    "    \n",
    "    dados_treinamento = tweets_embaralhados[:750]\n",
    "    dados_teste = tweets_embaralhados[751:]\n",
    "    \n",
    "    # treinando o algorítimo com os novos dados de treinamento\n",
    "    tabelas_relevancias, tabela_ingles = treinar(dados_treinamento)\n",
    "    \n",
    "    resultados_teste = testar(dados_teste, tabelas_relevancias)\n",
    "    \n",
    "    taxa_acerto = float(resultados_teste[0])\n",
    "    taxas_acerto.append(taxa_acerto)\n",
    "    \n",
    "    print(i + 1, end=': ')\n",
    "    print(taxa_acerto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4206976764662462\n",
      "0.4203625795175091\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.mean(taxas_acerto))\n",
    "print(np.median(taxas_acerto))\n",
    "plt.hist(taxas_acerto, density=True, edgecolor='white', bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Análise do histograma:</b> a partir do histograma, obsserva-se que as taxas de acerto variam bastante dependendo dos dados de treinamento, tornando o comportamento do classificador relativamente imprevisível sob esse aspecto. No entanto, maior parte das iterações de treinamento atingem uma taxa de acerto próxima "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou entre emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
